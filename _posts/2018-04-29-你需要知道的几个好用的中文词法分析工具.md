---
layout: post
title:  "你需要知道的几个好用的中文词法分析工具"
date:   2018-04-26
desc: "你需要知道的几个好用的中文词法分析工具"
keywords: "中文词法分析工具"
categories: [Deeplearning]
tags: [中文词法分析工具]
icon: icon-html
---

# **1、Stanford CoreNLP**

Stanford CoreNLP是**斯坦福大学自然语言处理小组**开发的自然语言分析工具集，包含**分句，分词，词性标注，命名实体识别，句法分析，指代消解，情感分析**等功能，这些工具采用流式(pipeline)集成方式，各功能模块之间相互解耦，提供单独的包下载，高度灵活且可扩展性强。Stanford CoreNLP支持**英文、阿拉伯文、中文、法文、德文以及西班牙文**。

**Stanford CoreNLP工具采用Java语言开发**，支持Java 8+。提供jar包下载，便于集成到Java代码中，同时也支持C#，Node.js，PHP，Python等语言调用。

Stanford CoreNLP的分词和命名实体识别工具是基于条件随机场模型实现的，而词性标注则是基于双向依存网络模型。

**官网介绍地址**：https://stanfordnlp.github.io/CoreNLP/index.html；

**论文地址**：https://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf；

**中文文本处理工具下载地址**：http://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-02-27-models.jar。

# **2、语言技术平台（Language Technology Platfor，LTP）**

LTP是**哈尔滨大学社会计算与信息检索研究中心**发布的一套中文自然语言处理工具集，包含**分句，分词，词性标注，命名实体识别，依存句法分析，语义角色标注，语义依存分析（树、图结构）**等工具。LTP同样采用流式集成方式，支持各功能模块单独调用。其中分词和词性标注等功能是基于**1998年人民日报和微博语料**进行训练，命名实体标注则是基于1998年人民日报语料进行训练。

**LTP采用C++语言开发**，提供静态、动态库，可方便集成在C，C++和Java程序中。

LTP的分词、词性标注以及命名实体识别工具是基于结构化感知器实现的。

LTP的使用非常简单，只需要**根据 API 参数构造 HTTP 请求即可在线获得分析结果**，而无需下载 SDK 、无需购买高性能的机器，同时支持跨平台、跨语言编程等。如果你需要使用LTP，则先在LTP上注册一个帐号，相关信息审核通过后，系统会给你发送API_KEY，并告知每个月的流量额度，初始是每月20G的免费流量。

语言云服务的API参数集如下表所示：

<table class="table table-bordered">
<thead>
<tr>
<th>参数名</th>
<th>含义</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>api_key</code></td>
<td>用户注册语言云服务后获得的认证标识</td>
<td></td>
</tr>
<tr>
<td><code>text</code></td>
<td>待分析的文本。</td>
<td>请以UTF-8格式编码，GET方式最大10K，POST方式最大20K</td>
</tr>
<tr>
<td><code>pattern</code></td>
<td>用以指定分析模式，可选值包括<code>ws</code>(分词)，<code>pos</code>(词性标注)，<code>ner</code>(命名实体识别)，<code>dp</code>(依存句法分析)，<code>sdp</code>(语义依存(树)分析)，<code>sdb_graph</code>(语义依存图分析)<code>srl</code>(语义角色标注),<code>all</code>(全部任务)</td>
<td>plain格式中不允许指定全部任务</td>
</tr>
<tr>
<td><code>format</code></td>
<td>用以指定结果格式类型，可选值包括<code>xml</code>(XML格式)，<code>json</code>(JSON格式)，<code>conll</code>(CONLL格式)，<code>plain</code>(简洁文本格式)</td>
<td>在指定pattern为all条件下，指定format为xml或json，返回结果将包含sdp结果，但conll格式不会包含sdp结果；</td>
</tr>
<tr>
<td><code>xml_input</code></td>
<td>用以指定输入text是否是xml格式，可选值为<code>false</code>(默认值),<code>true</code></td>
<td>仅限POST方式</td>
</tr>
<tr>
<td><code>has_key</code></td>
<td>用以指定json结果中是否含有键值，可选值包括<code>true</code>(含有键值，默认)，<code>false</code>(不含有键值)</td>
<td>配合format=json使用</td>
</tr>
<tr>
<td><code>only_ner</code></td>
<td>用以指定plain格式中是否只需要ner列表，可选值包括<code>false</code>(默认值)和<code>true</code></td>
<td>配合pattern=ner&amp;format=plain使用</td>
</tr>
<tr>
<td><code>callback</code></td>
<td>用以指定JavaScript调用中所使用的回调函数名称</td>
<td>配合format=json使用</td>
</tr>
</tbody>
</table>

**Python调用demo**

```
# -*- coding:utf8 -*-
if __name__ == '__main__':
    url_get_base = "http://api.ltp-cloud.com/analysis/"
    args = { 
        'api_key' : 'YourApiKey',
        'text' : '我是中国人。',
        'pattern' : 'dp',
        'format' : 'plain'
    }
    result = urllib.urlopen(url_get_base, urllib.urlencode(args)) # POST method
    content = result.read().strip()
    print content
```

**官网介绍地址**：http://www.ltp-cloud.com/intro/


# **3、清华大学词法分析器（THU Lexical Analyzer for Chinese，THULAC）**

THULAC由**清华大学自然语言处理与社会人文计算实验室**研制推出的一套中文词法分析工具包，具有**中文分词和词性标注**功能。THULAC工具包随包附带的分词模型Model_1以及分词和词性标注模型Model_2是由**人民日报语料库**训练得到的，所以支持单独调用分词接口，但不能单独调用词性标注接口。**语料库包含已标注的字数约为五千八百万字**，你可以填写申请表来得到全部语料库。

**THULAC完全开源，公布了算法源代码、算法模型甚至语料库**。在Python中，可通过 import thulac 来引用，同时还提供C++接口使用和按照命令格式运行可执行jar包完成分词和词性标注。

**C++版**

```
./thulac [-t2s] [-seg_only] [-deli delimeter] [-user userword.txt] 从命令行输入输出
./thulac [-t2s] [-seg_only] [-deli delimeter] [-user userword.txt] outputfile 利用重定向从文本文件输入输出（注意均为UTF8文本）
```

**java版**

```
java -jar THULAC_lite_java_run.jar [-t2s] [-seg_only] [-deli delimeter] [-user userword.txt] 从命令行输入输出
java -jar THULAC_lite_java_run.jar [-t2s] [-seg_only] [-deli delimeter] [-user userword.txt] -input input_file -output output_file 从文本文件输入输出（注意均为UTF8文本）
```

**python版（兼容python2.x和python3.x）**

通过python程序`import thulac`，新建thulac.thulac(args)类，其中args为程序的参数。之后可以通过调用thulac.cut()进行单句分词。

**官网介绍地址**：http://thulac.thunlp.org/

**相关论文地址**：https://dl.acm.org/citation.cfm?id=1667988.1667992

# **4、FudanNLP**

FudanNLP是**复旦大学自然语言处理实验室**开发的中文自然语言处理工具包，包含信息检索： **文本分类、新闻聚类；中文处理： 中文分词、词性标注、实体名识别、关键词抽取、依存句法分析、时间短语识别；结构化学习： 在线学习、层次分类、聚类**。FNLP也是开源的，其中包含了实现这些功能的机器学习算法和数据集。

**FNLP采用Java语言开发，可通过命令行方式调用，也可公布了jar包以便集成到各个Java项目中。**

**命令行方式调用分词功能**

```
java -Xmx1024m -Dfile.encoding=UTF-8 -classpath "fnlp-core/target/fnlp-core-2.1-SNAPSHOT.jar:libs/trove4j-3.0.3.jar:libs/commons-cli-1.2.jar" org.fnlp.nlp.cn.tag.CWSTagger -s models/seg.m "自然语言是人类交流和思维的主要工具，是人类智慧的结晶。"
```

参数`“-Xmx1024m”`设置Java虚拟机的可用内存为1024M。FNLP载入语言模型所需内存较大，因此可以利用此参数修改可用内存量。

**结果：**

> 自然 语言 是 人类 交流 和 思维 的 主要 工具 ， 是 人类 智慧 的 结晶 。

**命令行方式调用中文词性标注功能**

```
java -Xmx1024m -Dfile.encoding=UTF-8 -classpath "fnlp-core/target/fnlp-core-2.1-SNAPSHOT.jar:libs/trove4j-3.0.3.jar:libs/commons-cli-1.2.jar" org.fnlp.nlp.cn.tag.POSTagger -s models/seg.m models/pos.m "周杰伦出生于台湾，生日为79年1月18日，他曾经的绯闻女友是蔡依林。"
```

**结果：**

> 周杰伦/人名 出生/动词 于/介词 台湾/地名 ，/标点 生日/名词 为/介词 79年/时间短语 1月/时间短语 18日/时间短语 ，/标点
> 他/人称代词 曾经/副词 的/结构助词 绯闻/名词 女友/名词 是/动词 蔡依林/人名 。/标点

**命令行方式调用实体名识别功能**

```
java -Xmx1024m -Dfile.encoding=UTF-8 -classpath "fnlp-core/target/fnlp-core-2.1-SNAPSHOT.jar:libs/trove4j-3.0.3.jar:libs/commons-cli-1.2.jar" org.fnlp.nlp.cn.tag.NERTagger -s models/seg.m models/pos.m "詹姆斯·默多克和丽贝卡·布鲁克斯 鲁珀特·默多克旗下的美国小报《纽约邮报》的职员被公司律师告知，保存任何也许与电话窃听及贿赂有关的文件。"
```

**结果：**

>   {詹姆斯·默多克=人名, 鲁珀特·默多克旗=人名, 丽贝卡·布鲁克斯=人名, 纽约=地名, 美国=地名}

**FudanNLP gitbub地址（包含wiki）**：https://github.com/FudanNLP/fnlp
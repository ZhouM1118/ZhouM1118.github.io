---
layout: post
title:  "ASReader：一个经典的机器阅读理解深度学习模型"
date:   2018-05-09
desc: "ASReader：一个经典的机器阅读理解深度学习模型"
keywords: "ASReader, 机器阅读理解, 深度学习"
categories: [Deeplearning]
tags: [ASReader, 机器阅读理解, 深度学习]
icon: icon-html
---

# **0 前言**

前面的博文我们更多关注的是**机器阅读理解的数据集介绍以及基于传统机器学习方法的问答模型的建模**，随着2015年前后大量的大规模机器阅读理解数据集的发布（主要的机器阅读理解数据集前面博文有介绍），基于深度学习方法的机器阅读理解模型如雨后春笋，发展非常迅速，今天我们来介绍一种经典的**ASReader模型**，ASReader模型在2016年ACL会议上的《Text Understanding with the Attention Sum Reader Network 》提出。[论文地址](http://www.aclweb.org/anthology/P/P16/P16-1086.pdf)，[github源码地址](https://github.com/rkadlec/asreader)。

该论文提出了一个新的、简单的模型ASReader，它使用注意力机制从上下文中选择答案，而不是像在之前我们介绍的模型一样，使用文档与问题的相似度或提取特征构建特征工程等方式来定位答案。

# **1 数据集**

## **1.1 CNN&Daily Mail**

这个数据集我们在[《CNN&Dailymail：Teaching Machines to Read and Comprehend》](https://zhoum1118.github.io/deeplearning/2018/04/15/CNN&Dailymail-Teaching-Machines-to-Read-and-Comprehend.html)博文中介绍过，这是一个完形填空式的机器阅读理解数据集，从美国有线新闻网（CNN）和每日邮报网抽取近一百万篇文章，每篇文章作为一个文档（document），在文档的summary中剔除一个实体类单词，并作为问题（question），剔除的实体类单词即作为答案（answer），该文档中所有的实体类单词均可为候选答案（candidate answers）。其中每个样本使用命名实体识别方法将文本中所有的命名实体用类似“`@entity1`”替代，并随机打乱表示。

## **1.2 儿童故事（Children’s Book Test，CBT）**

从每一个儿童故事中提取20个连续的句子作为文档（document），第21个句子作为问题（question），并从中剔除一个实体类单词作为答案（answer）。机器阅读理解任务的复杂度往往随着答案的类型（命名实体、普通名词、动词、介词）变化而变化，标准LSTM语言模型在预测动词和介词类型的答案的性能基本可以达到人类的水平，但是它对命名实体和普通名词类型的答案的预测效果并不好。于是在这篇论文中**只关注命名实体和普通名词类型的答案的预测**。

图1展示了数据集的统计信息，其中CBT CN代表的是以普通名词为答案的CBT数据集，CBT NE代表的是以命名实体为答案的CBT数据集。

<img title="ASReader：一个经典的机器阅读理解深度学习模型_" 图片1="" src="https://img.mukewang.com/5af2dc19000142a018060430.png" alt="图片描述" style="display:block; margin:auto; width:70%">

<p style="text-align:center">图1 数据集的统计信息</p>

# **2 ASReader模型**

图2展示了ASReader模型框架图。

<img title="ASReader：一个经典的机器阅读理解深度学习模型_" 图片1="" src="https://img.mukewang.com/5af2dc300001c8fb14580806.png" alt="图片描述" style="display:block; margin:auto; width:60%">

<p style="text-align:center">图2 ASReader模型框架图</p>

## **2.1 embedding层**

该层主要功能为将文本转化为词向量表示方法，在图2中用e()表示。源码使用深度学习框架`Blocks`的`blocks.bricks.lookup.LookupTable`来构建：

> lookup = LookupTable(symbols_num, embedding_dims,
> weights_init=Uniform(width=0.2))

其中`symbols_num`为`LookupTable`的大小，这里初始化为`500`；`embedding_dims`为词向量维度，这里初始化为`256`。

## **2.2 Encode层**

这层的主要功能是对文档（document）和问题（question）进行编码（encode）。**对于文档的编码**，使用了一个单层双向GRU，对于文档中每个词用GRU正向隐藏层的输出与GRU反向隐藏层的输出的拼接向量来表示，这时候的拼接向量包含了这个词的上下文信息，在图2中使用`f()`表示；**对于问题的编码**，由于问题一般比文档要短很多，所以这时候将问题看作一个整体，同样使用了一个单层双向GRU来encode，但这时候不考虑每个词的隐藏层的输出，而是只考虑正向GRU最后的输出与反向GRU最后的输出，将这两个输出拼接作为问题的encode向量，在图2中使用`g()`表示。

## **2.3 点积（Dot products）**

将文档中每个编码后得到的word vector与问题编码后得到的question vector计算点积。

## **2.4 Softmax层**

这层主要将点积后的结果通过一个softmax函数，结果为注意力权重，代表**问题question与文档document中的每个词之前的相关性度量**。计算公式如下：

<img title="ASReader：一个经典的机器阅读理解深度学习模型_" 图片1="" src="https://img.mukewang.com/5af2dc4b0001001404480072.png" alt="图片描述" style="display:block; margin:auto; width:30%">

其中，`f_i(d)`表示文档中第`i`个词的encode后的向量表示，`g(q)`为问题encode后的向量表示，`s_i`为第`i`个单词为正确答案的概率。

## **2.5 累加**

候选答案词可能在文档中出现多次，**我们将候选答案词在文档出现地方的Softmax结果进行累加，得到每个候选答案词的概率**，最终最大概率的那个候选答案词即为正确答案answer。计算公式如下：

<img title="ASReader：一个经典的机器阅读理解深度学习模型_" 图片1="" src="https://img.mukewang.com/5af2dc5c00011b5d04780148.png" alt="图片描述" style="display:block; margin:auto; width:30%">

其中，`I(w, d)`表示文档`d`中的`w`出现的位置集合。

这里有个细节：**得到文档中每个单词为正确答案的概率做的是累加而不是累加后取平均值，这是因为作者分析实验语料库后发现一般（注意是一般，不是绝对）正确答案的单词在文档会出现更多次，取平均数可能会稀释总概率**。

# **3 实验结果及分析**

## **3.1 实验设置**

 - 优化函数：Adam
 - 学习率：0.001、0.0005
 - 损失函数：-logP(a|q, d)
 - embedding层权重矩阵初始化范围：[-0.1, 0.1]
 - GRU网络中的权值初始化：随机正交矩阵
 - GRU网络中的偏置初始化：0
 - batch size：32

**在模型中，`e，f，g`中的权重和偏置都需要训练**。

## **3.2 实验结果**

图3展示了模型对比实验结果。

<img title="ASReader：一个经典的机器阅读理解深度学习模型_" 图片1="" src="https://img.mukewang.com/5af2dc710001173213500952.png" alt="图片描述" style="display:block; margin:auto; width:50%">

<p style="text-align:center">图3 模型对比实验结果</p>

其中，`ASReader（avg for top 20%）`表示对超参数设计多种不同组合，并训练多个ASReader模型，取在验证集上效果最好的前20%个模型做bagging后得到的模型；`ASReader（avg ensemble）`取前效果最好的前70%的模型做bagging得到的模型；`ASReader（greedy ensemble）`每次取前效果最好的模型做bagging得到的模型。
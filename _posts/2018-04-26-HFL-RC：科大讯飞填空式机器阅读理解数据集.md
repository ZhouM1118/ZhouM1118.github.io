---
layout: post
title:  "HFL-RC：科大讯飞填空式机器阅读理解数据集"
date:   2018-04-26
desc: "HFL-RC：科大讯飞填空式机器阅读理解数据集"
keywords: "机器阅读理解, HFL-RC, 填空式数据集, 科大讯飞"
categories: [Deeplearning]
tags: [机器阅读理解, HFL-RC, 填空式数据集, 科大讯飞]
icon: icon-html
---

# **0 HFL-RC数据集**

**语料主要来源于人民日报和儿童童话**，[论文地址](https://arxiv.org/abs/1607.02250)。[数据集下载地址](https://github.com/ymcui/Chinese-RC-Dataset)。图1展示了从人民日报提取的样本案例。

<img title="HFL-RC：科大讯飞填空式机器阅读理解数据集_" 图片1="" src="https://img.mukewang.com/5ae135c900018c2a17320546.png" alt="图片描述" style="display:block; margin:auto; width:70%">

<p style="text-align:center">图 1 从人民日报提取的样本案例</p>

从图1中可以看出，文档包含了`序号1~10`的句子，每个句子以`“|||”`开头，**最后一个句子为问题和答案**，问题与答案用`“|||”`分割，如图1的`序号11`的句子，“策略”即为前面句子的答案。

同时，HTL-RC数据集要求**在文章中随机选择一个词作为答案，这个词必须为名词，且在文中至少出现两次**；那么显然，包含这个答案的句子就为问题，且**问题的来源是直接摘抄自文档中的一句话**。

为了保证数据的多样性，HTL-RC数据集同时还收集了儿童童话故事作为语料之一，儿童童话故事主要由动物故事或虚拟角色故事组成，这使得我们无法利用训练数据中的性别信息和大量背景知识，这对于解决多种类型的问题是很重要的。同时针对儿童童话故事语料，人工生成测试集，在实验中这比基于人民日报语料机器自动生成测试集要更难一些，因为自动生成的测试集可能更倾向于生成那些单词共现或固定搭配的句子，因此当这种搭配出现在问题的答案（在问题中答案用空格隐去）附近时，机器更容易识别正确答案。而在建立人共制造的测试集时，我们已经消除了这些类型的样本，这使得机器更难理解。直观地说，人工生成的测试集比先前发布的填空式风格的测试集都要难。

图2 展示了HFL-RC数据集的统计信息

<img title="HFL-RC：科大讯飞填空式机器阅读理解数据集_" 图片1="" src="https://img.mukewang.com/5ae135e40001b8f513980506.png" alt="图片描述" style="display:block; margin:auto; width:70%">

<p style="text-align:center">图 2 HFL-RC数据集的统计信息</p>

# **1 CAS Reader模型（Consensus Attention Sum Reader）**

CAS Reader模型将文档-问题-答案转化成三元组`<D, Q, A>`，模型主要受Kadlec的文献2启发，但CAS Reader模型考虑到了每个t时刻的RNN节点的输出，并从中挖掘相关联的信息，而不是仅仅考虑RNN层最后的输出。

**模型主要步骤如下：**

1、将文档`D`和问题`Q`分别用`one-hot向量`表示，并拼接两个向量；

2、文档`D`和问题`Q`共享嵌入层权值`W_e`。由于问题通常比文档短，通过共享文档和问题的嵌入层权值，可丰富问题的表示。

3、模型使用了两个双向RNN分别训练文档和问题的向量表示并拼接，训练后得到的向量表示就已经包含了过去和未来的上下文信息，计算公式如下所示。

<img title="HFL-RC：科大讯飞填空式机器阅读理解数据集_" 图片1="" src="https://img.mukewang.com/5ae135fa00015ce406300354.png" alt="图片描述" style="display:block; margin:auto; width:40%">

4、文档向量表示为`h_doc`，问题向量表示为`h_query`，向量为3维张量。现在**对于问题，模型计算在`t`时刻，每个文档单词的重要性，即注意力**。计算公式如下所示。

<img title="HFL-RC：科大讯飞填空式机器阅读理解数据集_" 图片1="" src="https://img.mukewang.com/5ae1360d0001486b06640076.png" alt="图片描述" style="display:block; margin:auto; width:40%">

其中`h_query(t)`表示t时刻问题的向量表示，计算其与`h_doc`的点积，使用`softmax函数`得到t时刻问题对文档的注意力分布`a(t)`，`a(t)`还可以表示为`a(t)=[a(t)_1, a(t)_2, …, a(t)_n]`，其中`a(t)_i`表示在`t`时刻，文档中第`i`个词的注意力值，`n`为文档单词个数。

5、合并问题的注意力分布，得到文档的最后注意力值`s`，计算公式如下所示。

<img title="HFL-RC：科大讯飞填空式机器阅读理解数据集_" 图片1="" src="https://img.mukewang.com/5ae136260001e92c04300070.png" alt="图片描述" style="display:block; margin:auto; width:30%">

其中`m`表示问题的单词个数，`f`表示合并函数，模型定义了三种合并函数，函数表示如下所示。

<img title="HFL-RC：科大讯飞填空式机器阅读理解数据集_" 图片1="" src="https://img.mukewang.com/5ae13641000192c309460322.png" alt="图片描述" style="display:block; margin:auto; width:50%">

6、计算单词`w`是答案的条件概率

文档`D`的单词组成单词空间`V`，单词`w`可能在单词空间`V`中出现了多次，其出现的位置`i`组成一个集合`I(w, D)`，对每个单词`w`，我们通过计算它的注意力值并求和得到单词`w`是答案的条件概率，计算公式如下所示。

<img title="HFL-RC：科大讯飞填空式机器阅读理解数据集_" 图片1="" src="https://img.mukewang.com/5ae136520001761706360128.png" alt="图片描述" style="display:block; margin:auto; width:40%">

CAS Reader模型结构图如图3所示。

<img title="HFL-RC：科大讯飞填空式机器阅读理解数据集_" 图片1="" src="https://img.mukewang.com/5ae136620001012a18480924.png" alt="图片描述" style="display:block; margin:auto; width:70%">

<p style="text-align:center">图 3 CAS Reader模型结构图</p>
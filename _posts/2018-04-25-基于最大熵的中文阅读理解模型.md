---
layout: post
title:  "基于最大熵的中文阅读理解模型"
date:   2018-04-25
desc: "基于最大熵的中文阅读理解模型"
keywords: "中文阅读理解模型, 最大熵, 主成分分析"
categories: [Deeplearning]
tags: [中文阅读理解模型, 最大熵, 主成分分析]
icon: icon-html
---

# **0 数据集**

本模型使用了山西大学构建的中文阅读理解语料库`CRCC（Chinese Reading Comprehension Corpus）V1.1`。该语料库包含121篇文章，其中80篇来自对外汉语教学，剩余41篇为在网络上爬取的文章，总共约6.5万。文章类型分为14类，其中包含地理、教育、科技、历史、生物等。平均每篇文章包含15个句子，3~6个问句，问句的类型包含`Q_HUMAN(人物，25个，占比4%)；Q_LOCATION(地点，51，8%)；Q_ENTITY(实体，78，13%)；Q_TIME(时间，53，9%)；Q_NUMBER(数值，78，13%)；Q_DISCRIBE(描述，332，54%)`。

**数据集中的文章已经进行了预处理，包括分词、词性标注、指称指代标注以及框架语义标注**，预处理具体方法可以参见文献1。

由于实验数据集规模较小，为了减少实验结果对训练集和测试集较强的依赖性，实验按照1：1的比例划分语料库，并尽可能使得各种类型的问句在训练集和测试集中比例接近1：1，训练集和测试集中各问题类型分布如图1所示。

<img title="基于最大熵的中文阅读理解模型_" 图片1="" src="https://img.mukewang.com/5adff1b9000108a616480568.png" alt="图片描述" style="display:block; margin:auto; width:70%">

<p style="text-align:center">图 1 训练集和测试集中各问题类型分布</p>

# **1 特征工程**

基于最大熵来构建中文阅读理解模型主要工作在于构建特征工程，特征抽取是机器学习中至关重要的步骤，**特征抽取的好坏直接影响模型的性能，其主要思想是将无法直接识别的原始数据转化为可识别的特征数据**。特征抽取是现实机器学习任务中的重要的“数据预处理”（data preprocessing）过程，通常先分析数据并进行特征抽取，然后再训练学习器。

实验的特征工程主要包含两种类型的特征：**词法特征和浅层句法特征。**

## **1.1 词法特征（10个）**

**DMWM：**统计问题q与其对应文章中句子s的词匹配个数m，取其中最大的m值记为M，那么每个句子的DMWM值则为（M-m）。其中DMWM为0时表示“问题-句子”的词匹配程度最高，在特殊情况下，比如当所有的m都为0，即问题q与文章所有句子的词匹配个数均为0，则DMWM均为0，在这种情况下我们将DMWM设置为200。

**DMVM：**统计问题q与其对应文章中句子s的动词匹配个数，计算方法同DMWM。

**DMWM-Prev：**统计问题q与其对应文章中句子s的前一个句子的词匹配个数，计算方法同DMWM。

**DMWM-Next：**统计问题q与其对应文章中句子s的后一个句子的词匹配个数，计算方法同DMWM。

**DMVM-Prev：**统计问题q与其对应文章中句子s的前一个句子的动词匹配个数，计算方法同DMWM。

**DMVM-Next：**统计问题q与其对应文章中句子s的后一个句子的动词匹配个数，计算方法同DMWM。

**PO：**表示句子s是否包含人（Person）、组织（Organization），有则设置为True，否则为False。

**DT：**表示句子s是否包含日期（Date）、时间（Time），有则设置为True，否则为False。

**LO：**表示句子s是否包含地点（Location），有则设置为True，否则为False。

**VA：**表示句子s是否包含数值（Value），有则设置为True，否则为False。

## **1.2 浅层句法特征（25个）**

**基本块类别特征：共8种**，包含：np（名词块），mp（数量块），sp（指代词块），tp（时间词块），vp（动词块），ap（形容词块），dp（副词块），pp（介词块）。实验分别抽取问句与答案句中的基本块类别进行匹配，如果基本块类别一致，且基本块的中心词一致，则匹配成功，匹配数递增加1，否则匹配失败，匹配数为0。

**基本块关系特征：共13种**，包含：ZX，LN，LH，PO，SB，AD，JB，AM，CD，RL，SX，XX，SG。实验分别抽取问句与答案句中的基本块关系标记进行匹配，如果基本块关系标记一致，且基本块的中心词一致，则匹配成功，匹配数递增加1，否则匹配失败，匹配数为0。

接下来我们使用一个样本案例来介绍基本块特征抽取过程。

<img title="基于最大熵的中文阅读理解模型_" 图片1="" src="https://img.mukewang.com/5adff1e1000112aa08220592.png" alt="图片描述" style="display:block; margin:auto; width:30%">

提取各基本块的中心词后，得到如下表示：

<img title="基于最大熵的中文阅读理解模型_" 图片1="" src="https://img.mukewang.com/5adff1ef00017bd608080266.png" alt="图片描述" style="display:block; margin:auto; width:30%">

对于基本块类别特征，匹配的基本块有两个：[人口/n    np-AM] 和 [分布/n    np-SG]，则对于该样本的基本块类别特征向量表示为`[2, 0, 0, 0, 0, 0, 0, 0]`。

对于基本块关系特征，匹配的基本块有两个：[人口/n    np-AM] 和 [分布/n    np-SG]，则对于该样本的基本块关系特征向量表示为`[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]`。

**功能块特征：共4种**，功能块表示一个中文句子的功能性成分，分析数据集，实验主要考虑了4种功能块：S（主语语块），P（谓语语块），O（宾语语块），D（状语语块），计算方法同基本块特征相似。

# **3 模型**

最大熵模型，即在满足约束条件的模型集合中选取熵最大的模型。其本质在于，**已知特征向量X，计算Y的概率，且尽可能让Y的概率最大**。

基于最大熵的中文阅读理解模型的主要任务是：在一篇文章中，找出概率最大的问题q对应的答案句A，也就是说**模型是在句子粒度上回答问题**。

那么，**中文阅读理解任务可以看出是一个二分类问题**，即给定文章S以及问题Q，对文章S中的n个句子进行二分类，假设“x”表示问题Q及文中的句子Si构成的上下文环境，“y”表示Si是否为答案句，那么基于最大熵模型进行建模可得：

$ P(y | x) = {1 \over Z(x)} e ^ {\sum_{i} \lambda_i f_i(x,y) } $

其中，$ Z(x) = \sum_{y}y e ^ {\sum_{i} \lambda_i f_i(x,y) }$为归一化因子，$f_i$为特征函数，$\lambda_i$为其权重。

# **4 实验结果及改进**

为了探索35个特征中不同组合特征对模型的影响程度，通过逐步增加特征的方式进行了实验。其中特征组合的方式有：**10个特征（词法特征），18个特征（词法特征+基本块类别特征），31个特征（词法特征+基本块类别特征+基本块关系特征），35个特征（词法特征+基本块类别特征+基本块关系特征+功能块特征）**。

图 2 展示了不同特征组合下模型在各个问题集的实验对比结果。

<img title="基于最大熵的中文阅读理解模型_" 图片1="" src="https://img.mukewang.com/5adff2290001dff916420510.png" alt="图片描述" style="display:block; margin:auto; width:70%">

<p style="text-align:center">图 2 (a) 不同特征组合下模型在各个问题集的HumSent对比结果</p>

<img title="基于最大熵的中文阅读理解模型_" 图片1="" src="https://img.mukewang.com/5adff2370001e16313160640.png" alt="图片描述" style="display:block; margin:auto; width:70%">

<p style="text-align:center">图 2 (b) 不同特征组合下模型在各个问题集的实验结果柱状图</p>

从图2可以看出，逐步加入特征并没有像我们预想的效果逐渐增加，而是**随着特征的增多，HumSent准确率有所下降**。分析原因有两个：第一是**数据集的预处理准确率不高**；第二是**数据集规模较小**，特征的增多，导致特征矩阵越来越稀疏。

基于以上实验结果以及分析原因，使用**主成分分析发对特征进行降维**，避开特征筛选过程，充分利用所有特征的有效信息来训练模型，使用降维后的特征向量构建主成分最大熵模型。

**主成分分析（Principal Component Analysis，PCA）**， 是一种统计方法。通过正交变换**将一组可能存在相关性的变量转换为一组线性不相关的变量**，转换后的这组变量叫主成分。

**少变量样本的研究中，我们可以很容易发现变量中的主成分**，因为变量少，变量之间的相关性弱，独立性强，我们可以很容易通过区分主成分变量来区分样本；多变量大样本虽然会为研究和应用提供了丰富的信息，但也在一定程度上增加了数据采集的工作量，更重要的是在多数情况下，**许多变量之间可能存在相关性，很难区分主成分变量，从而增加了问题分析的复杂性**，同时对分析带来不便。如果分别对每个指标进行分析，分析往往是孤立的，而不是综合的。盲目减少指标会损失很多信息，容易产生错误的结论。因此需要找到一个合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量间存在一定的相关关系，因此有可能**用较少的综合指标分别综合存在于各变量中的各类信息**。主成分分析与因子分析就属于这类降维的方法。

图3展示了主成分最大熵模型与非主成分最大熵模型实验对比结果。

<img title="基于最大熵的中文阅读理解模型_" 图片1="" src="https://img.mukewang.com/5adff2480001d66116460510.png" alt="图片描述" style="display:block; margin:auto; width:70%">

<p style="text-align:center">图 3 (a) 主成分最大熵模型与非主成分最大熵模型的HumSent对比结果</p>

<img title="基于最大熵的中文阅读理解模型_" 图片1="" src="https://img.mukewang.com/5adff2560001403213100636.png" alt="图片描述" style="display:block; margin:auto; width:70%">

<p style="text-align:center">图 3 (b) 主成分最大熵模型与非主成分最大熵模型实验柱状图</p>

# **参考文献**

 1. 张娜, 李济洪. 基于语义标注的中文阅读理解语料库的建设[C]// 全国计算语言学学术会议. 2007.
 2. 王凯华, 李济洪, 张国华,等. 基于最大熵模型的中文阅读理解问答系统技术研究[C]// 全国计算语言学学术会议. 2007.
 3. 李济洪, 王瑞波, 王凯华,等. 基于最大熵模型的中文阅读理解问题回答技术研究[J]. 中文信息学报, 2008, 22(6):55-62.